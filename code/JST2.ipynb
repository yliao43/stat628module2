{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sampleFromDirichlet(alpha):\n",
    "    return np.random.dirichlet(alpha)\n",
    "    \n",
    "    \n",
    "def sampleFromCategorical(theta):\n",
    "    theta = theta/np.sum(theta)\n",
    "    return np.random.multinomial(1, theta).argmax()\n",
    "\n",
    "\n",
    "class JST(object):\n",
    "    \n",
    "    def __init__(self, n_topics=5, beta=0.01, gamma=0.01, sentilab=2):\n",
    "        '''\n",
    "        alpha: for (sentiment, document)\n",
    "        beta: for (sentiment, topic)\n",
    "        gamma: for document\n",
    "        '''\n",
    "        self.n_topics = n_topics\n",
    "        self.alpha = 50/n_topics\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.sentilab = sentilab\n",
    "\n",
    "        \n",
    "    def read_corpus(self, corpus):\n",
    "\n",
    "        self.voc = set()\n",
    "        self.vocs = []\n",
    "        self.docs = []\n",
    "        self.n_docs = 0\n",
    "        corp = pd.read_csv(corpus,index_col=None,header=None)\n",
    "        corp = pd.np.array(corp.iloc[:,0]).tolist()\n",
    "        for doc in corp:\n",
    "            self.n_docs += 1\n",
    "            doc = doc.strip().split()[1:]\n",
    "            for w in doc:\n",
    "                self.voc.add(w)\n",
    "                self.vocs.append(w)\n",
    "            self.docs.append(doc)\n",
    "            self.doc_size = len(self.docs)\n",
    "            self.voc_size = len(self.voc)\n",
    "    \n",
    "    \n",
    "    def w_id(self):\n",
    "        \n",
    "        self.word2id = {}\n",
    "        self.id2word = {}\n",
    "        id=0\n",
    "        for w in self.voc:\n",
    "            self.word2id[w] = id\n",
    "            self.id2word[id] = w\n",
    "            id += 1\n",
    "\n",
    "    def read_score(self, pos_score):\n",
    "        \n",
    "        self.score = pd.read_csv(pos_score,index_col=None).to_dict()\n",
    "        \n",
    " \n",
    "    def _initialize_(self, corpus, pos_score):\n",
    "\n",
    "        self.read_corpus(corpus)\n",
    "        self.read_score(pos_score)\n",
    "        self.n_dt = np.zeros((self.n_docs, self.n_topics))\n",
    "        self.n_dts = np.zeros((self.n_docs, self.n_topics, self.sentilab))\n",
    "        self.n_d = np.zeros((self.n_docs))\n",
    "        self.n_vts = np.zeros((self.voc_size, self.n_topics, self.sentilab))\n",
    "        self.n_ts = np.zeros((self.n_topics, self.sentilab))\n",
    "        self.topics = {}\n",
    "        self.sentiments = {}\n",
    "        self.priorSentiment = {}\n",
    "\n",
    "        alphaVec = self.alpha * np.ones(self.n_topics)\n",
    "        gammaVec = self.gamma * np.ones(self.sentilab)\n",
    "        \n",
    "        for i, word in enumerate(self.voc):\n",
    "            posScore = self.score[word]\n",
    "            if posScore >= 0.1:\n",
    "                self.priorSentiment[i] = 1\n",
    "            elif posScore <= -0.1:\n",
    "                self.priorSentiment[i] = 0\n",
    "\n",
    "        for d in range(n_docs):\n",
    "\n",
    "            topicDistribution = sampleFromDirichlet(alphaVec)\n",
    "            sentimentDistribution = np.zeros((self.n_topics, self.sentilab))\n",
    "            for t in range(self.n_topics):\n",
    "                sentimentDistribution[t, :] = sampleFromDirichlet(gammaVec)\n",
    "            for i, w in enumerate(self.docs[d]):\n",
    "                t = sampleFromCategorical(topicDistribution)\n",
    "                s = sampleFromCategorical(sentimentDistribution[t, :])\n",
    "\n",
    "                self.topics[(d, i)] = t\n",
    "                self.sentiments[(d, i)] = s\n",
    "                self.n_dt[d, t] += 1\n",
    "                self.n_dts[d, t, s] += 1\n",
    "                self.n_d[d] += 1\n",
    "                self.n_vts[w, t, s] += 1\n",
    "                self.n_ts[t, s] += 1   \n",
    "                \n",
    "\n",
    "    def conditionalDistribution(self, d, v):\n",
    "        \"\"\"\n",
    "        Calculates the (topic, sentiment) probability for word v in document d\n",
    "        Returns:    a matrix (numTopics x numSentiments) storing the probabilities\n",
    "        \"\"\"\n",
    "        probabilities_ts = np.ones((self.n_topics, self.sentilab))\n",
    "        firstFactor = (self.n_dt[d] + self.alpha) / \\\n",
    "            (self.n_d[d] + self.n_topics * self.alpha)\n",
    "        secondFactor = (self.n_dts[d, :, :] + self.gamma) / \\\n",
    "            (self.n_dt[d, :] + self.sentilab * self.gamma)[:, np.newaxis]\n",
    "        thirdFactor = (self.n_vts[v, :, :] + self.beta) / \\\n",
    "            (self.n_ts + self.n_vts.shape[0] * self.beta)\n",
    "        probabilities_ts *= firstFactor[:, np.newaxis]\n",
    "        probabilities_ts *= secondFactor * thirdFactor\n",
    "        probabilities_ts /= np.sum(probabilities_ts)\n",
    "        return probabilities_ts\n",
    "\n",
    "    def getTopKWordsByLikelihood(self, K):\n",
    "        \"\"\"\n",
    "        Returns top K discriminative words for topic t and sentiment s\n",
    "        ie words v for which p(t, s | v) is maximum\n",
    "        \"\"\"\n",
    "        pseudocounts = np.copy(self.n_vts)\n",
    "        normalizer = np.sum(pseudocounts, (1, 2))\n",
    "        pseudocounts /= normalizer[:, np.newaxis, np.newaxis]\n",
    "        for t in range(self.n_topics):\n",
    "            for s in range(self.sentilab):\n",
    "                topWordIndices = pseudocounts[:, t, s].argsort()[-1:-(K + 1):-1]\n",
    "                vocab = self.voc\n",
    "                print( t, s, [vocab[i] for i in topWordIndices])\n",
    "\n",
    "    def getTopKWords(self, K):\n",
    "        \"\"\"\n",
    "        Returns top K discriminative words for topic t and sentiment s\n",
    "        ie words v for which p(v | t, s) is maximum\n",
    "        \"\"\"\n",
    "        pseudocounts = np.copy(self.n_vts)\n",
    "        normalizer = np.sum(pseudocounts, (0))\n",
    "        pseudocounts /= normalizer[np.newaxis, :, :]\n",
    "        for t in range(self.n_topics):\n",
    "            for s in range(self.sentilab):\n",
    "                topWordIndices = pseudocounts[:, t, s].argsort()[-1:-(K + 1):-1]\n",
    "                vocab = self.voc\n",
    "                print( t, s, [vocab[i] for i in topWordIndices])\n",
    "\n",
    "\n",
    "    def run(self, corpus, pos_score, maxIters=50):\n",
    "        \"\"\"\n",
    "        Runs Gibbs sampler for sentiment-LDA\n",
    "        \"\"\"\n",
    "        self._initialize_(corpus,pos_score)\n",
    "        print(\"Initialize done\")\n",
    "        self.w_id()\n",
    "        for iteration in range(maxIters):\n",
    "            print( \"Starting iteration %d of %d\" % (iteration + 1, maxIters))\n",
    "            for d in range(self.doc_size):\n",
    "                for i, v in enumerate(self.word2id(self.docs[d])):\n",
    "                    t = self.topics[(d, i)]\n",
    "                    s = self.sentiments[(d, i)]\n",
    "                    self.n_dt[d, t] -= 1\n",
    "                    self.n_d[d] -= 1\n",
    "                    self.n_dts[d, t, s] -= 1\n",
    "                    self.n_vts[v, t, s] -= 1\n",
    "                    self.n_ts[t, s] -= 1\n",
    "\n",
    "                    probabilities_ts = self.conditionalDistribution(d, v)\n",
    "                    if v in self.priorSentiment:\n",
    "                        s = self.priorSentiment[v]\n",
    "                        t = sampleFromCategorical(probabilities_ts[:, s])\n",
    "                    else:\n",
    "                        ind = sampleFromCategorical(probabilities_ts.flatten())\n",
    "                        t, s = np.unravel_index(ind, probabilities_ts.shape)\n",
    "\n",
    "                    self.topics[(d, i)] = t\n",
    "                    self.sentiments[(d, i)] = s\n",
    "                    self.n_dt[d, t] += 1\n",
    "                    self.n_d[d] += 1\n",
    "                    self.n_dts[d, t, s] += 1\n",
    "                    self.n_vts[v, t, s] += 1\n",
    "                    self.n_ts[t, s] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jst=JST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
